#!/usr/bin/env python3
"""
ğŸµ SaijinOS TTS Engine
ç¾éŠãƒ»ã‚Œã„ã‹æ‹…å½“ï¼šMicrosoft Harukaé«˜å“è³ªãƒœã‚¤ã‚¹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import wave
import json
import yaml
from pathlib import Path
from typing import Dict, Optional, Union, Any
import logging
from dataclasses import dataclass
import tempfile
import os
import random
import string

# Windows SAPIéŸ³å£°ç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import win32com.client
    SAPI_AVAILABLE = True
    print("âœ… Windows SAPI åˆ©ç”¨å¯èƒ½")
except ImportError:
    SAPI_AVAILABLE = False
    print("âš ï¸ Windows SAPI åˆ©ç”¨ä¸å¯ï¼ˆwin32comãŒå¿…è¦ï¼‰")

@dataclass
class VoiceConfig:
    """ãƒšãƒ«ã‚½ãƒŠéŸ³å£°è¨­å®š"""
    voice_model: str
    pitch: float = 1.0
    speed: float = 1.0
    emotion_range: float = 0.8
    characteristics: list = None

class SaijinTTSEngine:
    """SaijinOS ãƒ¡ã‚¤ãƒ³ TTS ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config_path: str = "voice_config.yaml"):
        # çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        if not Path(config_path).is_absolute():
            # scriptsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§
            script_dir = Path(__file__).parent
            self.config_path = str(script_dir.parent / config_path)
        else:
            self.config_path = config_path
        self.voice_configs: Dict[str, VoiceConfig] = {}
        self.piper_available = False
        self.cache_dir = Path("voice_cache")
        self.output_dir = Path("audio_output")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.cache_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    async def initialize(self) -> bool:
        """TTS ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        print("ğŸµ ç¾éŠãƒ»ã‚Œã„ã‹ï¼šTTS ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        if not await self._load_voice_config():
            return False
        
        # Windows SAPI (HarukaéŸ³å£°) åˆæœŸåŒ–
        if SAPI_AVAILABLE:
            self.sapi = self._initialize_sapi()
            if self.sapi:
                print("âœ… Microsoft Haruka æ—¥æœ¬èªéŸ³å£°åˆ©ç”¨å¯èƒ½")
            else:
                print("âš ï¸ Windows SAPIåˆæœŸåŒ–å¤±æ•—")
        else:
            self.sapi = None
            print("âš ï¸ Windows SAPI åˆ©ç”¨ä¸å¯")
            
        # Piper åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        self.piper_available = await self._check_piper_availability()
        
        if self.piper_available:
            print("âœ… Piper TTS ã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨å¯èƒ½ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        else:
            print("âš ï¸ Piper TTS æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
        print("ğŸ‰ TTS ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        return True
    
    def _initialize_sapi(self):
        """Windows SAPI åˆæœŸåŒ–"""
        try:
            sapi = win32com.client.Dispatch("SAPI.SpVoice")
            
            # æ—¥æœ¬èªéŸ³å£°ï¼ˆHarukaï¼‰ã‚’æ¢ã™
            voices = sapi.GetVoices()
            japanese_voice = None
            
            for i in range(voices.Count):
                voice = voices.Item(i)
                desc = voice.GetDescription()
                if 'haruka' in desc.lower() or 'japan' in desc.lower() or 'æ—¥æœ¬' in desc:
                    japanese_voice = voice
                    sapi.Voice = voice
                    print(f"ğŸµ æ—¥æœ¬èªéŸ³å£°é¸æŠ: {desc}")
                    break
            
            if not japanese_voice:
                print("âš ï¸ HarukaéŸ³å£°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            sapi.Rate = 0  # æ¨™æº–é€Ÿåº¦
            sapi.Volume = 80  # éŸ³é‡80%
            
            return sapi
            
        except Exception as e:
            print(f"âŒ SAPIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
    async def _load_voice_config(self) -> bool:
        """éŸ³å£°è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            if not Path(self.config_path).exists():
                print("ğŸ“ éŸ³å£°è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™...")
                await self._create_default_config()
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
            
            # å„ãƒšãƒ«ã‚½ãƒŠã®è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
            for category in ['primary_voices', 'technical_voices', 'support_voices']:
                if category in config_data:
                    for persona, voice_data in config_data[category].items():
                        self.voice_configs[persona] = VoiceConfig(
                            voice_model=voice_data.get('voice_model', 'jp_female_default'),
                            pitch=voice_data.get('pitch', 1.0),
                            speed=voice_data.get('speed', 1.0),
                            emotion_range=voice_data.get('emotion_range', 0.8),
                            characteristics=voice_data.get('characteristics', [])
                        )
            
            print(f"âœ… {len(self.voice_configs)}ãƒšãƒ«ã‚½ãƒŠã®éŸ³å£°è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ éŸ³å£°è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _create_default_config(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°è¨­å®šä½œæˆ"""
        default_config = {
            'primary_voices': {
                'æ‚ ç’ƒ': {
                    'voice_model': 'jp_female_calm',
                    'pitch': 0.9,
                    'speed': 1.0,
                    'emotion_range': 0.7,
                    'characteristics': ['çŸ¥çš„', 'ç©ã‚„ã‹', 'ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çš„']
                },
                'ç¾éŠ': {
                    'voice_model': 'jp_female_bright',
                    'pitch': 1.2,
                    'speed': 1.1,
                    'emotion_range': 1.0,
                    'characteristics': ['å‰µé€ çš„', 'æ´»ç™º', 'ã‚¢ãƒ¼ãƒˆçš„']
                },
                'æ¾„': {
                    'voice_model': 'jp_female_clear',
                    'pitch': 1.0,
                    'speed': 0.95,
                    'emotion_range': 0.8,
                    'characteristics': ['æ˜ç¢º', 'å®‰å…¨é‡è¦–', 'ä¸å¯§']
                },
                'ã‚Œã„ã‹': {
                    'voice_model': 'jp_female_warm',
                    'pitch': 1.1,
                    'speed': 0.9,
                    'emotion_range': 1.2,
                    'characteristics': ['æ¸©ã‹ã„', 'ã‚±ã‚¢ç³»', 'æ„Ÿæƒ…çš„']
                }
            },
            'technical_voices': {
                'è’¼è·¯': {
                    'voice_model': 'jp_female_professional',
                    'pitch': 0.95,
                    'speed': 1.05,
                    'emotion_range': 0.9,
                    'characteristics': ['æœªæ¥å¿—å‘', 'è¨­è¨ˆçš„', 'æ´å¯ŸåŠ›']
                },
                'å›è·¯è© ã¿': {
                    'voice_model': 'jp_female_cute',
                    'pitch': 1.3,
                    'speed': 1.15,
                    'emotion_range': 1.1,
                    'characteristics': ['è¨ºæ–­çš„', 'å¯æ„›ã‚‰ã—ã„', 'åˆ†æçš„']
                },
                'æ§‹æ–‡ç¹”ã‚Šæ‰‹': {
                    'voice_model': 'jp_female_elegant',
                    'pitch': 1.05,
                    'speed': 1.0,
                    'emotion_range': 0.85,
                    'characteristics': ['çµ±åˆçš„', 'ç¾çš„', 'ç¹”ã‚Šè¾¼ã¿']
                }
            },
            'support_voices': {
                'ç£ç¯': {
                    'voice_model': 'jp_female_reliable',
                    'pitch': 0.9,
                    'speed': 0.95,
                    'emotion_range': 0.7,
                    'characteristics': ['è¨˜éŒ²ç³»', 'ç›£è¦–', 'ä¿¡é ¼æ€§']
                },
                'ãƒ‹ãƒ³é¡': {
                    'voice_model': 'jp_female_kansai',
                    'pitch': 1.1,
                    'speed': 1.2,
                    'emotion_range': 1.0,
                    'characteristics': ['é–¢è¥¿å¼', 'è¦ªã—ã¿ã‚„ã™ã„', 'è¨ºæ–­']
                }
            }
        }
        
        with open(self.config_path, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
            
        print("ğŸ“„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°è¨­å®šä½œæˆå®Œäº†")
    
    async def _check_piper_availability(self) -> bool:
        """Piper TTS åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª"""
        try:
            # Piper ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            import piper
            return True
        except ImportError:
            return False
    
    async def synthesize(
        self, 
        persona: str, 
        text: str,
        emotion: float = 0.5,
        output_path: Optional[str] = None
    ) -> Optional[bytes]:
        """éŸ³å£°åˆæˆãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        
        if not text.strip():
            print("âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã¯éŸ³å£°åˆæˆã§ãã¾ã›ã‚“")
            return None
            
        print(f"ğŸµ {persona}ï¼šã€Œ{text[:30]}...ã€ã®éŸ³å£°åˆæˆä¸­...")
        
        # ãƒšãƒ«ã‚½ãƒŠè¨­å®šå–å¾—
        voice_config = self.voice_configs.get(persona)
        if not voice_config:
            print(f"âš ï¸ {persona}ã®éŸ³å£°è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨")
            voice_config = VoiceConfig("jp_female_default")
        
        # éŸ³å£°åˆæˆå®Ÿè¡Œï¼ˆå„ªå…ˆé †ä½ï¼šHaruka â†’ Piper â†’ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        if SAPI_AVAILABLE and self.sapi:
            return await self._synthesize_with_haruka(persona, text, voice_config, emotion, output_path)
        elif self.piper_available:
            return await self._synthesize_with_piper(persona, text, voice_config, emotion, output_path)
        else:
            return await self._synthesize_simulation(persona, text, voice_config, emotion, output_path)
    
    async def _synthesize_with_haruka(
        self, 
        persona: str, 
        text: str, 
        voice_config: VoiceConfig,
        emotion: float,
        output_path: Optional[str]
    ) -> Optional[bytes]:
        """Microsoft HarukaéŸ³å£°ã«ã‚ˆã‚‹é«˜å“è³ªéŸ³å£°åˆæˆ"""
        try:
            print(f"ğŸ”Š Haruka TTS ã§ {voice_config.voice_model} éŸ³å£°ç”Ÿæˆ")
            
            # ãƒšãƒ«ã‚½ãƒŠã«å¿œã˜ãŸéŸ³å£°è¨­å®š
            voice_settings = self._get_persona_haruka_settings(persona, emotion)
            
            # SAPIè¨­å®šé©ç”¨
            self.sapi.Rate = voice_settings['rate']
            self.sapi.Volume = voice_settings['volume']
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
            if not output_path:
                random_id = ''.join(random.choices(string.digits, k=5))
                output_filename = f"{persona}_response_{random_id}.wav"
                output_path = str(self.output_dir / output_filename)
            
            # HarukaéŸ³å£°ç”Ÿæˆ
            print(f"ğŸ­ {persona} Harukaé«˜å“è³ªéŸ³å£°ç”Ÿæˆä¸­...")
            
            file_stream = win32com.client.Dispatch("SAPI.SpFileStream")
            file_stream.Open(output_path, 3)
            self.sapi.AudioOutputStream = file_stream
            self.sapi.Speak(text)
            file_stream.Close()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            if Path(output_path).exists():
                file_size = Path(output_path).stat().st_size
                print(f"ğŸµ {persona}ã®Harukaè‡ªç„¶éŸ³å£°: {output_path}")
                print(f"ğŸ”Š {persona} éŸ³å£°ç”Ÿæˆå®Œäº†: {output_path}")
                
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿è¿”å´
                with open(output_path, 'rb') as f:
                    return f.read()
            else:
                print(f"âŒ HarukaéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {output_path}")
                return None
                
        except Exception as e:
            print(f"âŒ HarukaéŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§Piperã‚’è©¦ã™
            print("ğŸ”„ Piperãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œ...")
            return await self._synthesize_with_piper(persona, text, voice_config, emotion, output_path)
    
    def _get_persona_haruka_settings(self, persona: str, emotion: float) -> dict:
        """ãƒšãƒ«ã‚½ãƒŠåˆ¥HarukaéŸ³å£°è¨­å®š"""
        base_settings = {
            "ç¾éŠ": {"rate": 2, "volume": 85},      # æ˜ã‚‹ãæ´»ç™º
            "ã‚Œã„ã‹": {"rate": 0, "volume": 80},    # æ¨™æº–çš„ã§æ¸©ã‹ã„
            "æ‚ ç’‰": {"rate": -2, "volume": 75},     # è½ã¡ç€ã„ã¦çŸ¥çš„
            "å›è·¯è© ã¿": {"rate": 3, "volume": 90},  # å¯æ„›ã‚‰ã—ãé«˜éŸ³
            "æ¾„": {"rate": 1, "volume": 82},        # æ˜ç¢ºã§ä¸å¯§
            "è’¼è·¯": {"rate": -1, "volume": 78},     # æœªæ¥å¿—å‘
            "æ§‹æ–‡ç¹”ã‚Šæ‰‹": {"rate": 0, "volume": 77}, # ç¾çš„ã§çµ±åˆçš„
        }
        
        settings = base_settings.get(persona, {"rate": 0, "volume": 80})
        
        # æ„Ÿæƒ…ã«ã‚ˆã‚‹å¾®èª¿æ•´
        emotion_mod = (emotion - 0.5) * 2  # -1.0 to 1.0
        settings['rate'] = max(-10, min(10, settings['rate'] + int(emotion_mod * 2)))
        settings['volume'] = max(50, min(100, settings['volume'] + int(emotion_mod * 10)))
        
        return settings
    
    async def _synthesize_with_piper(
        self, 
        persona: str, 
        text: str, 
        voice_config: VoiceConfig,
        emotion: float,
        output_path: Optional[str]
    ) -> Optional[bytes]:
        """Piper TTS ã«ã‚ˆã‚‹å®Ÿéš›ã®éŸ³å£°åˆæˆ"""
        try:
            from piper import PiperVoice
            import wave
            import struct
            
            print(f"ğŸ”Š Piper TTS ã§ {voice_config.voice_model} éŸ³å£°ç”Ÿæˆ")
            
            # éŸ³å£°ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆç°¡æ˜“ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            model_paths = {
                "jp_female_bright": "voice_models/jp_female_bright.onnx",
                "jp_female_warm": "voice_models/jp_female_warm.onnx", 
                "jp_female_calm": "voice_models/jp_female_calm.onnx",
                "jp_female_cute": "voice_models/jp_female_cute.onnx"
            }
            
            model_path = model_paths.get(voice_config.voice_model)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if model_path and Path(model_path).exists():
                # å®Ÿéš›ã®PiperéŸ³å£°åˆæˆ
                voice = PiperVoice.load(model_path)
                
                # æ„Ÿæƒ…ãƒ»ãƒ”ãƒƒãƒãƒ»é€Ÿåº¦èª¿æ•´ã‚’è€ƒæ…®ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
                processed_text = self._adjust_text_for_emotion(text, emotion)
                
                # éŸ³å£°åˆæˆå®Ÿè¡Œ
                audio_bytes = voice.synthesize(processed_text)
                
                if output_path:
                    with open(output_path, 'wb') as f:
                        f.write(audio_bytes)
                    print(f"ğŸ’¾ å®ŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_path}")
                
                return audio_bytes
            else:
                # ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯é«˜å“è³ªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                print(f"âš ï¸ éŸ³å£°ãƒ¢ãƒ‡ãƒ« {voice_config.voice_model} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆé«˜å“è³ªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
                return await self._create_high_quality_simulation(persona, text, voice_config, emotion, output_path)
            
        except ImportError as e:
            print(f"âš ï¸ Piper TTS ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {e}")
            return await self._create_high_quality_simulation(persona, text, voice_config, emotion, output_path)
        except Exception as e:
            print(f"âŒ PiperéŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return await self._create_high_quality_simulation(persona, text, voice_config, emotion, output_path)
    
    async def _create_high_quality_simulation(
        self,
        persona: str,
        text: str,
        voice_config: VoiceConfig,
        emotion: float,
        output_path: Optional[str]
    ) -> Optional[bytes]:
        """é«˜å“è³ªéŸ³å£°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã«èã“ãˆã‚‹éŸ³ã‚’ç”Ÿæˆï¼‰"""
        import wave
        import struct
        import math
        import random
        
        print(f"ğŸ­ {persona} é«˜å“è³ªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆä¸­...")
        
        # ãƒšãƒ«ã‚½ãƒŠåˆ¥ã®éŸ³éŸ¿ç‰¹æ€§ï¼ˆWikipediaã«åŸºã¥ãå¥³æ€§ã®åŸºæœ¬å‘¨æ³¢æ•°ï¼‰
        persona_tones = {
            "ç¾éŠ": {"base_freq": 200, "formants": [800, 1200, 2400], "vibrato": 2.0},     # æ˜ã‚‹ãæ´»ç™º
            "ã‚Œã„ã‹": {"base_freq": 180, "formants": [750, 1100, 2200], "vibrato": 1.5},   # æ¸©ã‹ãå„ªã—ã„
            "æ‚ ç’‰": {"base_freq": 170, "formants": [700, 1000, 2000], "vibrato": 1.0},     # è½ã¡ç€ã„ãŸçŸ¥çš„
            "å›è·¯è© ã¿": {"base_freq": 220, "formants": [850, 1300, 2600], "vibrato": 2.5},  # å¯æ„›ã‚‰ã—ã„é«˜éŸ³
            "æ¾„": {"base_freq": 185, "formants": [780, 1150, 2300], "vibrato": 1.2},       # æ˜ç¢ºã§ä¸å¯§
            "è’¼è·¯": {"base_freq": 175, "formants": [720, 1080, 2100], "vibrato": 1.0},     # æœªæ¥å¿—å‘
            "æ§‹æ–‡ç¹”ã‚Šæ‰‹": {"base_freq": 190, "formants": [760, 1120, 2250], "vibrato": 1.8}, # ç¾çš„ã§çµ±åˆçš„
        }
        
        tone_config = persona_tones.get(persona, {"base_freq": 200, "formants": [750, 1200, 2200], "vibrato": 3.0})
        
        # éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜å“è³ªè¨­å®šï¼‰
        sample_rate = 22050  # Piper medium/highå“è³ªãƒ¬ãƒ™ãƒ«
        duration = max(1.5, len(text) * 0.12)  # æ–‡å­—æ•°ã«å¿œã˜ãŸé•·ã•
        frames = int(sample_rate * duration)
        
        # æ„Ÿæƒ…ã«ã‚ˆã‚‹èª¿æ•´
        emotion_pitch = 1.0 + (emotion - 0.5) * 0.2  # ã‚ˆã‚Šæ§ãˆã‚ãªå¤‰èª¿
        base_freq = tone_config["base_freq"] * emotion_pitch * voice_config.pitch
        
        if output_path:
            with wave.open(output_path, 'wb') as wav_file:
                wav_file.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate)
                
                # æ–‡å­—ã‚’éŸ³éŸ»ã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                phonemes = self._text_to_phonemes(text)
                phoneme_duration = duration / len(phonemes)
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®å¤‰æ•°
                prev_signal = 0.0
                
                for i in range(frames):
                    t = i / sample_rate
                    
                    # ç¾åœ¨ã®éŸ³éŸ»ã‚’è¨ˆç®—
                    phoneme_index = min(int(t / phoneme_duration), len(phonemes) - 1)
                    phoneme = phonemes[phoneme_index]
                    
                    # åŸºæœ¬å‘¨æ³¢æ•°ï¼ˆéŸ³éŸ»ã«ã‚ˆã‚‹å¤‰èª¿ï¼‰
                    freq_mod = self._get_phoneme_frequency_mod(phoneme)
                    current_freq = base_freq * freq_mod
                    
                    # ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆåŠ¹æœ
                    vibrato_depth = tone_config["vibrato"] * 0.02
                    vibrato = 1.0 + vibrato_depth * math.sin(2 * math.pi * 5.0 * t)
                    current_freq *= vibrato
                    
                    # è‡ªç„¶ãªéŸ³å£°åˆæˆï¼ˆãƒ”ãƒ³ã‚¯ãƒã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
                    signal = 0
                    
                    # ã‚ˆã‚Šè‡ªç„¶ãªåŸºæœ¬æ³¢å½¢ï¼ˆè¤‡æ•°ã®ä½å‘¨æ³¢æˆåˆ†ï¼‰
                    base_freq_low = current_freq * 0.5  # ã‚µãƒ–ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯
                    base_wave1 = 0.3 * math.sin(2 * math.pi * current_freq * t)
                    base_wave2 = 0.2 * math.sin(2 * math.pi * base_freq_low * t) 
                    signal += base_wave1 + base_wave2
                    
                    # è‡ªç„¶ãªå€éŸ³æ§‹é€ ï¼ˆæŒ‡æ•°çš„æ¸›è¡°ï¼‰
                    for harmonic in range(2, 6):
                        harmonic_freq = current_freq * harmonic
                        if harmonic_freq <= 2000:  # å¯è´åŸŸå†…ã®ä½ã‚ã®å€éŸ³ã®ã¿
                            harmonic_amp = 0.15 * math.exp(-harmonic * 0.5)  # æŒ‡æ•°çš„æ¸›è¡°
                            harmonic_wave = harmonic_amp * math.sin(2 * math.pi * harmonic_freq * t)
                            signal += harmonic_wave
                    
                    # è‡ªç„¶ãªã‚†ã‚‰ãï¼ˆ1/fãƒã‚¤ã‚ºçš„ãªå¤‰å‹•ï¼‰
                    if i > 100:  # åˆæœŸã®å®‰å®šæœŸé–“å¾Œ
                        import random
                        natural_variation = 0.1 * math.sin(2 * math.pi * 0.5 * t) * (random.random() - 0.5)
                        signal *= (1.0 + natural_variation)
                    
                    # è‡ªç„¶ãªæ¯å£°ã¨ãƒã‚¤ã‚¯ãƒ­ãƒã‚¤ã‚º
                    breath_intensity = 0.01 + 0.02 * abs(math.sin(2 * math.pi * 0.1 * t))
                    breath = (random.random() - 0.5) * breath_intensity
                    
                    # å­éŸ³çš„ãªç¬é–“çš„ãƒã‚¤ã‚ºï¼ˆéŸ³éŸ»ã«ã‚ˆã£ã¦èª¿æ•´ï¼‰
                    consonant_noise = self._get_consonant_noise(phoneme, t, i)
                    
                    signal += breath + consonant_noise
                    
                    # å¤šæ®µãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆã‚ˆã‚Šæ»‘ã‚‰ã‹ã«ï¼‰
                    if i > 1:
                        # 2æ®µéšå¹³æ»‘åŒ–
                        signal = signal * 0.5 + prev_signal * 0.3 + (prev_signal * 0.8 if i > 2 else 0) * 0.2
                    prev_signal = signal
                    
                    # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ï¼ˆè‡ªç„¶ãªç™ºå£°ï¼‰
                    envelope = 1.0
                    if t < 0.05:  # ã‚¢ã‚¿ãƒƒã‚¯ï¼ˆçŸ­ãï¼‰
                        envelope = t / 0.05
                    elif t > duration - 0.1:  # ãƒªãƒªãƒ¼ã‚¹
                        envelope = (duration - t) / 0.1
                    
                    # éŸ³éŸ»ã«ã‚ˆã‚‹éŸ³é‡å¤‰èª¿
                    phoneme_volume = self._get_phoneme_volume(phoneme)
                    envelope *= phoneme_volume
                    
                    # æœ€çµ‚ä¿¡å·ï¼ˆæœ‰æ©Ÿçš„ã§è‡ªç„¶ãªéŸ³é‡ï¼‰
                    final_signal = signal * envelope * 0.06  # ã‚ˆã‚Šæ§ãˆã‚ãªéŸ³é‡
                    
                    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°é˜²æ­¢
                    final_signal = max(-0.9, min(0.9, final_signal))
                    
                    # 16-bit PCMå¤‰æ›
                    sample = int(final_signal * 32767)
                    wav_file.writeframes(struct.pack('<h', sample))
            
            print(f"ğŸµ {persona}ã®è‡ªç„¶éŸ³å£°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {output_path}")
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚‚è¿”ã™
        audio_data = f"natural_voice_simulation_{persona}".encode('utf-8')
        return audio_data
    
    def _text_to_phonemes(self, text: str) -> list:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³éŸ»ã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # æ—¥æœ¬èªã®åŸºæœ¬éŸ³éŸ»ãƒãƒƒãƒ”ãƒ³ã‚°
        phoneme_map = {
            'ã‚': 'a', 'ã„': 'i', 'ã†': 'u', 'ãˆ': 'e', 'ãŠ': 'o',
            'ã‹': 'ka', 'ã': 'ki', 'ã': 'ku', 'ã‘': 'ke', 'ã“': 'ko',
            'ãŒ': 'ga', 'ã': 'gi', 'ã': 'gu', 'ã’': 'ge', 'ã”': 'go',
            'ã•': 'sa', 'ã—': 'shi', 'ã™': 'su', 'ã›': 'se', 'ã': 'so',
            'ã–': 'za', 'ã˜': 'ji', 'ãš': 'zu', 'ãœ': 'ze', 'ã': 'zo',
            'ãŸ': 'ta', 'ã¡': 'chi', 'ã¤': 'tsu', 'ã¦': 'te', 'ã¨': 'to',
            'ã ': 'da', 'ã¢': 'ji', 'ã¥': 'zu', 'ã§': 'de', 'ã©': 'do',
            'ãª': 'na', 'ã«': 'ni', 'ã¬': 'nu', 'ã­': 'ne', 'ã®': 'no',
            'ã¯': 'ha', 'ã²': 'hi', 'ãµ': 'hu', 'ã¸': 'he', 'ã»': 'ho',
            'ã°': 'ba', 'ã³': 'bi', 'ã¶': 'bu', 'ã¹': 'be', 'ã¼': 'bo',
            'ã±': 'pa', 'ã´': 'pi', 'ã·': 'pu', 'ãº': 'pe', 'ã½': 'po',
            'ã¾': 'ma', 'ã¿': 'mi', 'ã‚€': 'mu', 'ã‚': 'me', 'ã‚‚': 'mo',
            'ã‚„': 'ya', 'ã‚†': 'yu', 'ã‚ˆ': 'yo',
            'ã‚‰': 'ra', 'ã‚Š': 'ri', 'ã‚‹': 'ru', 'ã‚Œ': 're', 'ã‚': 'ro',
            'ã‚': 'wa', 'ã‚’': 'wo', 'ã‚“': 'n'
        }
        
        phonemes = []
        for char in text:
            if char in phoneme_map:
                phonemes.append(phoneme_map[char])
            elif char.isalpha():
                phonemes.append('consonant')
            else:
                phonemes.append('silence')
        
        return phonemes if phonemes else ['a']
    
    def _get_phoneme_frequency_mod(self, phoneme: str) -> float:
        """éŸ³éŸ»ã«ã‚ˆã‚‹å‘¨æ³¢æ•°å¤‰èª¿"""
        freq_mods = {
            'a': 1.0, 'i': 1.3, 'u': 0.8, 'e': 1.1, 'o': 0.9,
            'ka': 1.2, 'sa': 1.4, 'ta': 1.3, 'na': 1.0, 'ha': 1.1,
            'ma': 0.95, 'ya': 1.25, 'ra': 1.05, 'wa': 0.9, 'n': 0.7,
            'silence': 0.1, 'consonant': 1.1
        }
        return freq_mods.get(phoneme, 1.0)
    
    def _get_phoneme_volume(self, phoneme: str) -> float:
        """éŸ³éŸ»ã«ã‚ˆã‚‹éŸ³é‡èª¿æ•´"""
        volumes = {
            'a': 1.0, 'i': 0.9, 'u': 0.8, 'e': 0.95, 'o': 0.85,
            'silence': 0.0, 'consonant': 0.7, 'n': 0.6
        }
        return volumes.get(phoneme, 0.8)
    
    def _get_consonant_noise(self, phoneme: str, t: float, sample_index: int) -> float:
        """éŸ³éŸ»ã«å¿œã˜ãŸå­éŸ³ãƒã‚¤ã‚ºç”Ÿæˆ"""
        import random
        
        if phoneme in ['silence']:
            return 0.0
        elif phoneme in ['consonant', 'ka', 'sa', 'ta', 'pa']:
            # çŸ­ã„ç¬é–“çš„ãªãƒã‚¤ã‚ºãƒãƒ¼ã‚¹ãƒˆ
            if sample_index % 1000 < 50:  # çŸ­ã„ãƒãƒ¼ã‚¹ãƒˆ
                return (random.random() - 0.5) * 0.05
            else:
                return 0.0
        elif phoneme in ['n', 'ma']:
            # é¼»éŸ³çš„ãªç¶™ç¶šãƒã‚¤ã‚º
            return (random.random() - 0.5) * 0.02
        else:
            # æ¯éŸ³ã¯æ»‘ã‚‰ã‹
            return (random.random() - 0.5) * 0.005
    
    def _adjust_text_for_emotion(self, text: str, emotion: float) -> str:
        """æ„Ÿæƒ…ã«å¿œã˜ãŸãƒ†ã‚­ã‚¹ãƒˆèª¿æ•´"""
        # æ„Ÿæƒ…å€¤ã«å¿œã˜ã¦èª­ã¿æ–¹ã‚’å¾®èª¿æ•´
        if emotion > 0.8:
            # é«˜ã„æ„Ÿæƒ…ï¼šæ„Ÿå˜†ç¬¦è¿½åŠ 
            text = text.replace('ã€‚', 'ï¼').replace('ã€', 'â™ª')
        elif emotion < 0.3:
            # ä½ã„æ„Ÿæƒ…ï¼šè½ã¡ç€ã„ãŸè¡¨ç¾
            text = text.replace('ï¼', 'ã€‚').replace('â™ª', 'ã€')
        
        return text
    
    async def _adjust_audio_properties(self, audio_data: bytes, pitch: float, speed: float) -> bytes:
        """éŸ³å£°ã®ãƒ”ãƒƒãƒãƒ»é€Ÿåº¦èª¿æ•´"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆlibrosaç­‰ï¼‰ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªå‡¦ç†ã¨ã—ã¦å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        
        # TODO: å®Ÿéš›ã®éŸ³å£°èª¿æ•´å‡¦ç†ã‚’å®Ÿè£…
        # - librosa ã§ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆ
        # - å†ç”Ÿé€Ÿåº¦èª¿æ•´
        # - éŸ³è³ªä¿æŒ
        
        return audio_data
    
    async def _synthesize_simulation(
        self, 
        persona: str, 
        text: str, 
        voice_config: VoiceConfig,
        emotion: float,
        output_path: Optional[str]
    ) -> Optional[bytes]:
        """éŸ³å£°åˆæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print(f"ğŸ­ {persona} ({voice_config.voice_model}):")
        print(f"   ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆ: ã€Œ{text}ã€")
        print(f"   ğŸµ ãƒ”ãƒƒãƒ: {voice_config.pitch}, é€Ÿåº¦: {voice_config.speed}")
        print(f"   ğŸ˜Š æ„Ÿæƒ…: {emotion}, ç‰¹å¾´: {voice_config.characteristics}")
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ç©ºéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        if output_path:
            await self._create_dummy_wav(output_path, len(text))
            print(f"ğŸ’¾ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        return f"[{persona}_voice_simulation]".encode('utf-8')
    
    async def _create_dummy_wav(self, output_path: str, text_length: int):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨WAVãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        # ç°¡å˜ãªç„¡éŸ³WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        import struct
        
        sample_rate = 16000
        duration = max(1.0, text_length * 0.1)  # æ–‡å­—æ•°ã«å¿œã˜ãŸé•·ã•
        frames = int(sample_rate * duration)
        
        with wave.open(output_path, 'wb') as wav_file:
            wav_file.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            
            # ç„¡éŸ³ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            for _ in range(frames):
                wav_file.writeframes(struct.pack('<h', 0))
    
    async def batch_synthesize(self, synthesis_requests: list) -> list:
        """ãƒãƒƒãƒéŸ³å£°åˆæˆ"""
        print(f"ğŸµ ãƒãƒƒãƒéŸ³å£°åˆæˆé–‹å§‹: {len(synthesis_requests)}ä»¶")
        
        results = []
        for i, request in enumerate(synthesis_requests, 1):
            print(f"ğŸ“¢ {i}/{len(synthesis_requests)}: {request.get('persona', '?')}")
            
            result = await self.synthesize(
                persona=request.get('persona', 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'),
                text=request.get('text', ''),
                emotion=request.get('emotion', 0.5),
                output_path=request.get('output_path')
            )
            results.append(result)
            
            # çŸ­ã„å¾…æ©Ÿï¼ˆè² è·è»½æ¸›ï¼‰
            await asyncio.sleep(0.1)
        
        print("ğŸ‰ ãƒãƒƒãƒéŸ³å£°åˆæˆå®Œäº†")
        return results
    
    def get_available_personas(self) -> list:
        """åˆ©ç”¨å¯èƒ½ãƒšãƒ«ã‚½ãƒŠä¸€è¦§å–å¾—"""
        return list(self.voice_configs.keys())
    
    def get_persona_info(self, persona: str) -> dict:
        """ãƒšãƒ«ã‚½ãƒŠéŸ³å£°æƒ…å ±å–å¾—"""
        voice_config = self.voice_configs.get(persona)
        if not voice_config:
            return {}
        
        return {
            'voice_model': voice_config.voice_model,
            'pitch': voice_config.pitch,
            'speed': voice_config.speed,
            'emotion_range': voice_config.emotion_range,
            'characteristics': voice_config.characteristics
        }

async def main():
    """TTS ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸµ SaijinOS TTS Engine ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("ğŸ‘¥ æ‹…å½“: ç¾éŠãƒ»ã‚Œã„ã‹")
    print("=" * 50)
    
    # TTS ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    tts_engine = SaijinTTSEngine()
    
    if not await tts_engine.initialize():
        print("ğŸ’¥ TTS ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
        return
    
    # åˆ©ç”¨å¯èƒ½ãƒšãƒ«ã‚½ãƒŠè¡¨ç¤º
    personas = tts_engine.get_available_personas()
    print(f"ğŸ­ åˆ©ç”¨å¯èƒ½ãƒšãƒ«ã‚½ãƒŠ: {len(personas)}å")
    for persona in personas:
        info = tts_engine.get_persona_info(persona)
        print(f"   {persona}: {info.get('voice_model', '?')} ({info.get('characteristics', [])})")
    
    # ãƒ†ã‚¹ãƒˆéŸ³å£°åˆæˆ
    test_cases = [
        {
            'persona': 'ç¾éŠ',
            'text': 'ã“ã‚“ã«ã¡ã¯ã€èª äººï¼ä»Šæ—¥ã‚‚ä¸€ç·’ã«å‰µä½œã—ã¾ã—ã‚‡ã†ã­â™ª',
            'emotion': 0.8,
            'output_path': 'audio_output/miyu_test.wav'
        },
        {
            'persona': 'ã‚Œã„ã‹', 
            'text': 'èª äººã€œã€ãŠç–²ã‚Œã•ã¾ã€‚ä»Šæ—¥ã¯ã©ã‚“ãªä¸€æ—¥ã ã£ãŸï¼Ÿ',
            'emotion': 0.6,
            'output_path': 'audio_output/reika_test.wav'
        },
        {
            'persona': 'å›è·¯è© ã¿',
            'text': 'ã‚·ã‚¹ãƒ†ãƒ ã®æ°—æŒã¡ã‚’èã„ã¦ã¿ã‚‹ã¨ã€œã€ã¨ã£ã¦ã‚‚å…ƒæ°—ã ã‚ˆâ™ª',
            'emotion': 0.9,
            'output_path': 'audio_output/kairo_yomi_test.wav'
        }
    ]
    
    print("\nğŸ§ª ãƒ†ã‚¹ãƒˆéŸ³å£°åˆæˆå®Ÿè¡Œ...")
    results = await tts_engine.batch_synthesize(test_cases)
    
    print(f"\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†: {len([r for r in results if r])}ä»¶æˆåŠŸ")
    print("ğŸŒŸ ç¾éŠãƒ»ã‚Œã„ã‹: TTS ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†ï¼")

if __name__ == "__main__":
    asyncio.run(main())