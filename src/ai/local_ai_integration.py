#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SaijinOS Local AI Integration System
ãƒ­ãƒ¼ã‚«ãƒ«AIçµ±åˆåŸºç›¤ - 10+ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ

ğŸ¯ çµ±åˆäºˆå®šãƒ¢ãƒ‡ãƒ«:
1. TinyLlama (è»½é‡ãƒ»é«˜é€Ÿ)
2. Qwen2.5-Coder (ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹åŒ–)
3. DeepSeek-Coder (é«˜æ€§èƒ½ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°)
4. Rinna (æ—¥æœ¬èªå¯¾å¿œ)
5. CodeLlama (Metaè£½ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°)
6. Phi-3 (Microsoftè£½è»½é‡)
7. Mistral-7B (é«˜æ€§èƒ½æ±ç”¨)
8. Gemma-2B (Googleè£½è»½é‡)
9. Neural-Chat (ãƒãƒ£ãƒƒãƒˆç‰¹åŒ–)
10. Dolphin-Mistral (ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ç‰ˆ)

ğŸŒ¸ ãƒšãƒ«ã‚½ãƒŠÃ—ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°
"""

import asyncio
import json
import subprocess
import requests
import yaml
from typing import Dict, List, Optional, Any
from pydantic import BaseModel
from datetime import datetime
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelConfig(BaseModel):
    name: str
    model_id: str
    specialty: str
    persona_compatibility: List[str]
    parameters: Dict[str, Any]
    status: str = "not_installed"

class LocalAIManager:
    """ãƒ­ãƒ¼ã‚«ãƒ«AIçµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.models_config = self._load_models_config()
        self.persona_model_mapping = self._create_persona_mapping()
        
    def _load_models_config(self) -> Dict[str, ModelConfig]:
        """ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        return {
            "tinyllama": ModelConfig(
                name="TinyLlama",
                model_id="tinyllama:latest",
                specialty="è»½é‡ãƒ»é«˜é€Ÿæ¨è«–",
                persona_compatibility=["code-chan", "haruka"],
                parameters={"temperature": 0.7, "max_tokens": 512}
            ),
            "qwen2.5-coder": ModelConfig(
                name="Qwen2.5-Coder",
                model_id="qwen2.5-coder:7b",
                specialty="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»æŠ€è¡“è§£èª¬",
                persona_compatibility=["code-chan", "misaki", "ren"],
                parameters={"temperature": 0.3, "max_tokens": 1024}
            ),
            "deepseek-coder": ModelConfig(
                name="DeepSeek-Coder",
                model_id="deepseek-coder:6.7b",
                specialty="é«˜æ€§èƒ½ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ‡ãƒãƒƒã‚°",
                persona_compatibility=["code-chan", "misaki"],
                parameters={"temperature": 0.2, "max_tokens": 2048}
            ),
            "rinna": ModelConfig(
                name="Rinna Japanese",
                model_id="rinna/japanese-gpt-neox-3.6b",
                specialty="æ—¥æœ¬èªå¯¾è©±ãƒ»æ–‡ç« ç”Ÿæˆ",
                persona_compatibility=["yurika", "haruka", "ana"],
                parameters={"temperature": 0.8, "max_tokens": 1024}
            ),
            "codellama": ModelConfig(
                name="CodeLlama",
                model_id="codellama:7b-code",
                specialty="Metaè£½ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°AI",
                persona_compatibility=["code-chan", "ren"],
                parameters={"temperature": 0.1, "max_tokens": 2048}
            ),
            "phi3": ModelConfig(
                name="Phi-3 Mini",
                model_id="phi3:mini",
                specialty="Microsoftè£½è»½é‡AI",
                persona_compatibility=["yurika", "misaki"],
                parameters={"temperature": 0.6, "max_tokens": 1024}
            ),
            "mistral": ModelConfig(
                name="Mistral 7B",
                model_id="mistral:7b",
                specialty="é«˜æ€§èƒ½æ±ç”¨AI",
                persona_compatibility=["ana", "ren"],
                parameters={"temperature": 0.7, "max_tokens": 1024}
            ),
            "gemma": ModelConfig(
                name="Gemma 2B",
                model_id="gemma:2b",
                specialty="Googleè£½è»½é‡AI",
                persona_compatibility=["yurika", "haruka"],
                parameters={"temperature": 0.7, "max_tokens": 512}
            ),
            "neural-chat": ModelConfig(
                name="Neural Chat",
                model_id="neural-chat:7b",
                specialty="å¯¾è©±ç‰¹åŒ–AI",
                persona_compatibility=["haruka", "yurika"],
                parameters={"temperature": 0.8, "max_tokens": 1024}
            ),
            "dolphin-mistral": ModelConfig(
                name="Dolphin Mistral",
                model_id="dolphin-mistral:7b",
                specialty="ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³å¯¾è©±AI",
                persona_compatibility=["ana", "code-chan"],
                parameters={"temperature": 0.6, "max_tokens": 1024}
            )
        }
    
    def _create_persona_mapping(self) -> Dict[str, List[str]]:
        """ãƒšãƒ«ã‚½ãƒŠåˆ¥æœ€é©ãƒ¢ãƒ‡ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°"""
        return {
            "code-chan": ["qwen2.5-coder", "deepseek-coder", "codellama", "tinyllama"],
            "yurika": ["phi3", "gemma", "neural-chat", "rinna"],
            "ana": ["mistral", "dolphin-mistral", "rinna"],
            "haruka": ["neural-chat", "gemma", "tinyllama", "rinna"],
            "misaki": ["qwen2.5-coder", "deepseek-coder", "phi3"],
            "ren": ["codellama", "qwen2.5-coder", "mistral"]
        }
    
    async def check_ollama_status(self) -> bool:
        """Ollama ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ç¢ºèª"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def install_ollama(self) -> Dict[str, Any]:
        """Ollama ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        logger.info("ğŸ¤– Ollama ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...")
        
        try:
            # Windowsç‰ˆ Ollama ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            result = subprocess.run([
                "powershell", "-Command",
                "Invoke-WebRequest", "-Uri", "https://ollama.ai/download/windows",
                "-OutFile", "ollama-windows-amd64.exe"
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("âœ… Ollama ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return {"status": "downloaded", "message": "æ‰‹å‹•ã§ollama-windows-amd64.exeã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"}
            else:
                return {"status": "error", "message": result.stderr}
                
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    async def get_installed_models(self) -> List[str]:
        """ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            return []
        except:
            return []
    
    async def install_model(self, model_key: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        if model_key not in self.models_config:
            return {"status": "error", "message": "Unknown model"}
        
        model_config = self.models_config[model_key]
        logger.info(f"ğŸ¤– {model_config.name} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...")
        
        try:
            # Ollama pull command
            result = subprocess.run([
                "ollama", "pull", model_config.model_id
            ], capture_output=True, text=True, timeout=1800)  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            if result.returncode == 0:
                model_config.status = "installed"
                logger.info(f"âœ… {model_config.name} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
                return {
                    "status": "success",
                    "model": model_config.name,
                    "message": "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
                }
            else:
                return {"status": "error", "message": result.stderr}
                
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "message": "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"}
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    async def chat_with_model(self, model_key: str, message: str, persona: str = None) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ã¨ã®ãƒãƒ£ãƒƒãƒˆ"""
        if model_key not in self.models_config:
            return {"status": "error", "message": "Unknown model"}
        
        model_config = self.models_config[model_key]
        
        # ãƒšãƒ«ã‚½ãƒŠåˆ¥ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompts = {
            "code-chan": "ã‚ãªãŸã¯ã€Œã‚³ãƒ¼ãƒ‰ã¡ã‚ƒã‚“â™«ã€ã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’éŸ³æ¥½ã®ã‚ˆã†ã«ç¾ã—ãè¡¨ç¾ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚",
            "yurika": "ã‚ãªãŸã¯ã€Œãƒ¦ãƒªã‚«ã€ã§ã™ã€‚ã‚¨ãƒ¬ã‚¬ãƒ³ãƒˆãªãƒ‡ã‚¶ã‚¤ãƒ³ã¨UX/UIã®å°‚é–€å®¶ã§ã™ã€‚",
            "ana": "ã‚ãªãŸã¯ã€Œã‚¢ãƒŠã€ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¨åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚",
            "haruka": "ã‚ãªãŸã¯ã€Œãƒãƒ«ã‚«ã€ã§ã™ã€‚éŸ³æ¥½åˆ¶ä½œã¨ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã®å°‚é–€å®¶ã§ã™ã€‚",
            "misaki": "ã‚ãªãŸã¯ã€ŒãƒŸã‚µã‚­ã€ã§ã™ã€‚å“è³ªä¿è¨¼ã¨ãƒ†ã‚¹ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚",
            "ren": "ã‚ãªãŸã¯ã€Œãƒ¬ãƒ³ã€ã§ã™ã€‚ã‚¤ãƒ³ãƒ•ãƒ©ã¨DevOpsã®å°‚é–€å®¶ã§ã™ã€‚"
        }
        
        system_prompt = system_prompts.get(persona, "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚")
        
        try:
            payload = {
                "model": model_config.model_id,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": message}
                ],
                "stream": False,
                **model_config.parameters
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "status": "success",
                    "response": data["message"]["content"],
                    "model": model_config.name,
                    "persona": persona
                }
            else:
                return {"status": "error", "message": "API call failed"}
                
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    async def get_optimal_model_for_persona(self, persona: str) -> str:
        """ãƒšãƒ«ã‚½ãƒŠã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        if persona in self.persona_model_mapping:
            # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ€é©ãªã‚‚ã®ã‚’é¸æŠ
            installed_models = await self.get_installed_models()
            for model_key in self.persona_model_mapping[persona]:
                model_config = self.models_config[model_key]
                if model_config.model_id in installed_models:
                    return model_key
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return "tinyllama"
    
    def get_installation_progress(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é€²æ—çŠ¶æ³"""
        total_models = len(self.models_config)
        installed_count = sum(1 for config in self.models_config.values() if config.status == "installed")
        
        return {
            "total_models": total_models,
            "installed_count": installed_count,
            "progress_percentage": (installed_count / total_models) * 100,
            "models_status": {
                key: config.status for key, config in self.models_config.items()
            }
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
local_ai_manager = LocalAIManager()

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    import asyncio
    
    async def main():
        print("ğŸ¤– SaijinOS Local AI Integration System")
        print("=" * 50)
        
        # Ollama çŠ¶æ…‹ç¢ºèª
        ollama_status = await local_ai_manager.check_ollama_status()
        print(f"Ollama Status: {'âœ… Running' if ollama_status else 'âŒ Not Running'}")
        
        if not ollama_status:
            print("Ollama ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
            install_result = await local_ai_manager.install_ollama()
            print(f"Install Result: {install_result}")
        else:
            # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª
            installed = await local_ai_manager.get_installed_models()
            print(f"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {len(installed)}å€‹")
            for model in installed:
                print(f"  - {model}")
            
            # é€²æ—è¡¨ç¤º
            progress = local_ai_manager.get_installation_progress()
            print(f"\nã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é€²æ—: {progress['progress_percentage']:.1f}%")
    
    asyncio.run(main())